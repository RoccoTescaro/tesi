\chapter{Esperimenti}\label{ch:chapter2}

Per la valutazione delle metriche sono stati condotti diversi esperimenti che possono essere suddivisi in due macro-categorie dipendentemente dal tipo di dataset utilizzato:
\begin{itemize}
    \item \textbf{Toy Dataset}: dataset generati artificialmente per testare il comportamento delle metriche in condizioni controllate.
    \item \textbf{Real World Dataset}: dataset reali per testare il comportamento delle metriche in condizioni reali.
\end{itemize}
Le sezioni seguenti descrivono come sono state implementate le metriche, gli esperimenti così come le distribuzioni di dati utilizzate.\
La ragione per cui sono stati condotti esperimenti su dateset generati artificilmente (ovvero di matrice matematica, non derivanti dalla realtà o generati da reti neurali) è che in questo modo è stato possibile osservare il comportamento delle metriche in condizioni controllate, ideali e per poter confrontare i risultati ottenuti con quelli attesi presenti nella letteratura.\
Fanno infatti parte dei test su 'toy dataset', i test di corretta implementazione ovvero un'analisi comparativa delle diverse implementazioni in codice delle metriche. Tali test hanno il fine di verificare che le metriche restituiscano valori corretti validando gli altri esperimenti.\

I dataset reali sono stati utilizzati per testare il comportamento delle metriche in condizioni reali, ovvero per verificare se le metriche si comportano come ci si aspetta in situazioni non ben definite, con distribuzioni di dati non note, ben distanti da quelle ideali. Questo infatti, come vedremo, può sollevare criticità, ad esempio dovute alla assenza di certe categorie di dati o alla presenza di dati non ben distribuiti.\
Un altro elemento dei dataset reali è che spesso questi non sono di carattere prettamente numerico, nei dataset analizzati ad esempio, abbiamo immagini di farfalle e partiture musicali. Questo comporta la necessità di trasformare i dati per poterli utilizzare, e il processo di \textbf{estrazioni di caratteristiche} può equivalere ad una perdita di dati significativi o, al contrario, ad una sovrabbondanza di dati non significativi.\

\section{Toy Dataset}

Come anticipato nell’introduzione di questo capitolo, i Toy Dataset sono dataset generati artificialmente tramite funzioni matematiche che producono dati in modo casuale ma controllato. Per questi esperimenti, sono state utilizzate principalmente due tipo di distribuzioni di dati numerici con dimensione variabile: la distribuzione uniforme e la distribuzione normale. In una specifica categoria di esperimenti, sono stati aggiunti anche outliers, ossia dati che si discostano significativamente dalla distribuzione principale, con l'obiettivo di valutare la robustezza delle metriche in presenza di anomalie.

Gli esperimenti condotti si ispirano a quelli presenti in letteratura e hanno diversi obiettivi, tra cui:
\begin{itemize}
    \item Testare l'influenza degli iperparametri delle metriche sul loro comportamento.
    \item Valutare la risposta delle metriche alla presenza di outliers.
    \item Studiare l'impatto della dimensione del dataset sui risultati.
    \item Confrontare le implementazioni delle metriche esistenti, sia in termini di valori scalari che tramite curve di precision-recall (PR-curve).
\end{itemize}

Per ciascun esperimento sono stati prodotti grafici che illustrano i risultati ottenuti. L'unica eccezione è rappresentata dai test di corretta implementazione delle metriche scalari, dove la validazione è stata condotta principalmente tramite confronti numerici. La scelta delle tipologie di grafici è stata guidata dall'obiettivo di facilitare il confronto con i risultati riportati in letteratura.

Data la complessità computazionale di alcuni esperimenti, i risultati intermedi e finali sono stati salvati in file \texttt{.npy}, permettendo analisi approfondite e riproducibilità senza dover ripetere calcoli onerosi. Questo approccio non solo consente una gestione efficiente dei dati, ma permette anche un'analisi successiva più flessibile, ad esempio per esplorare ulteriori correlazioni o per verificare ipotesi aggiuntive.

\subsection{Parametro k e dimensione del dataset}

Come abbiamo visto, tutte le metriche analizzate si basano sulla distanza dei dati rispetto ai loro vicini. Uno dei parametri più determinanti è l'ordine \texttt{k} del vicino più prossimo. Secondo la letteratura, i valori ottimali di \texttt{k} variano in base alla metrica analizzata: per l’\textbf{improved precision recall} \texttt{k = 3}, per la \textbf{probabilistic precision recall} \texttt{k = 4}, per la \textbf{precision recall coverage} \texttt{k=\(\sqrt{|\Phi|}\)} indicando con \(|\Phi|\) la dimensione del dataset reale e/o generato (o \texttt{k = 3} se \texttt{C = 3})
, mentre per \textbf{density} e \textbf{coverage} \texttt{k = 5}. Questi valori riflettono un compromesso ottimale tra stabilità della metrica e sensibilità alla densità locale. Ci si aspetta che l'aumento della dimensione del dataset porti a un incremento dei valori delle metriche, poiché una maggiore quantità di dati aumenta la densità dei punti, migliorando la rappresentatività delle distribuzioni e riducendo l’effetto del rumore.

L'analisi è stata condotta su dataset generati una sola volta, mantenuti costanti e identici per la valutazione di tutte le metriche, questo per garantire coerenza nei risultati. Sono state utilizzate, come anticipato precedentemente, due diverse distribuzioni: uniforme e normale. I test sono stati effettuati per valori di \texttt{k} variabili da 1 a 10 vale a dire \([[1,2,3,\dots,10]\) e per dimensioni del dataset crescenti esponenzialmente, da 500 a 16000 punti (\([500,1000,2000,\dots,16000]\)). Ogni dato è rappresentato come un vettore in \(\mathbb{R}^{64}\).

Per presentare i risultati, sono state utilizzate delle \textbf{heatmap}, che mostrano i valori delle metriche in funzione di \texttt{k} e della dimensione del dataset. In queste rappresentazioni, il rosso indica valori prossimi a \texttt{1.}, mentre il verde valori vicini a \texttt{0.}. Sebbene le heatmap non offrano precisione numerica immediata, forniscono una visione d’insieme sulle tendenze generali delle metriche e facilitano il confronto con gli esperimenti presenti in letteratura \cite{3ReliableFidelityDiversityMetrics}. Nel paper di riferimento, sono state prodotte heatmap solo per le metriche di \textbf{improved precision recall} e \textbf{density and coverage}, ma per completezza sono state prodotte anche per le altre metriche presentate nel capitolo precedente.

\subsection{Dimensione del dataset e dimensione dei dati}

In questo caso, l'analisi condotta non ha un riscontro diretto nella letteratura esistente, ovvero non presenta antecedenti (quantomeno per gli articoli presi in analisi). L'obiettivo è determinare come la \textbf{dimensione del dataset} possa influenzare la densità dei dati della distribuzione (a dimensione dei dati fissata) e, conseguentemente, il valore delle metriche. I risultati di questa analisi costituiscono una parte fondamentale per le analisi su dati reali, dove la scelta del numero di caratteristiche da considerare può risultare determinante.

In questo esperimento, abbiamo considerato una distribuzione normale (\(\mathcal{N}(0, I)\)) dei dati con \textbf{dimensione del dataset variabile} da 50 a 1600, con valori \([50, 100, 200, 400, 800, 1600]\), e \textbf{dimensione dei dati} da 2 a 64, con valori \([2, 4, 8, 16, 32, 64]\). 
A differenza degli esperimenti precedenti, rappresentati tramite heatmap, qui l'assenza di un riscontro nella letteratura ci ha permesso di utilizzare \textbf{grafici a linee bidimensionali} per rappresentare i risultati (data la ridotta dimensionalità, almeno per le dimensioni di interesse, di uno degli iperparametri da regolare, ovvero la dimensione dei dati).
Il valore dell'\textbf{iperparametro \( k \)} è stato scelto in accordo con quanto suggerito nei vari articoli, per garantire la massima efficacia della metrica. Le misurazioni sono state ripetute 25 volte e successivamente mediate. Anche in questo caso, il calcolo è stato effettuato in parallelo.

\subsection{Outliers}

Una delle proprietà più rilevanti da esaminare nelle diverse metriche è la loro \textbf{resistenza agli outliers}. In linea con la letteratura, abbiamo analizzato come i valori delle metriche cambiano in presenza di dataset con distribuzione normale, \textbf{senza outliers} e con \textbf{outliers} inseriti sia nei dati reali sia in quelli generati.

L’esperimento si è svolto considerando una distribuzione reale fissa \( X \sim \mathcal{N}(0, I) \) e una distribuzione generata \( Y \sim \mathcal{N}(\mu, I) \), con uno shift della media \(\mu\) variabile in \([-1,1]\) (con \textbf{step} di \texttt{0.05}). In aggiunta, sono stati esaminati due scenari di outliers, in cui un campione estremo a \( x = +1 \) è stato aggiunto ai dati reali o a quelli generati. 
Lo spazio di lavoro è stato definito in \( \mathbb{R}^{64} \), con vettori reali centrati sull’origine e campioni generati con media variabile lungo la direzione del vettore unitario.
Come per gli altri esperimenti condotti, le operazioni sono state svolte in \textbf{parallelo}.

Oltre alle metriche di \textbf{precision-recall} e \textbf{density-coverage} (come nel paper), i test sono stati condotti anche sulle metriche di \textbf{probabilistic precision-recall} e \textbf{improved precision-recall}. Per ciascuna metrica sono stati utilizzati i valori degli iperparametri suggeriti dai rispettivi articoli.

In assenza di outliers, ci si attende che i valori delle metriche diminuiscano gradualmente man mano che \(\mu\) si allontana da zero, indicando correttamente la divergenza tra le due distribuzioni.

\subsection{Comparazione con implementazioni esistenti}

Non tutti i papers analizzati presentavano un'implementazione delle metriche in codice. Sono stati svolti dei test confrontando su dataset identici le diverse implementazioni delle metriche presenti in letteratura.\
Non sono stati possibili confronti diretti per quanto riguarda la \textbf{precision-recall coverage}, in quanto non sono state trovate implementazioni in codice, mentre per la \textbf{improved precision-recall} sono state confrontate due diverse implementazioni.\
Sono state scelte tre diverse distribuzioni di dati: distribuzione uniforme, distribuzione normale e distribuzione normale con media in \(3/\sqrt{dim}\). Ciascuna distribuzione è stata generata con dimensione del dataset pari a 10000 e dimensione dei dati pari a 64.\
I risultati sono stati riportati su un file \texttt{log.txt} per poter essere confrontati in un secondo momento.\
Allo scopo di velocizzare la computazione e ridurre il tempo di esecuzione, i test comparativi per la \textbf{probabilistic precision-recall} e la \textbf{density-coverage} sono stati eseguiti con ordine \( k = 3 \), nonostante i valori ottimali suggeriti dalla letteratura siano diversi. Questo hai infatti permesso di calcolare le distanze intraset una sola volta, evitando di ripeterle per ogni valore di \( k \) e dato che non eravamo interessati a valutare l'efficacia delle metriche, quanto confrontare le diverse implementazioni.
#TODO Aggiungere bibliografia per le implementazioni in codice delle metriche.

\subsection{Riproduzione delle pr-curve}

Anche per la riproduzione delle pr-curve sono stati utilizzati dataset generati artificialmente. L'articolo di riferimento per questo esperimento \cite{pr-curve}, non presentava un'implementazione in codice delle metriche, ma solo i risultati ottenuti.\
Abbiamo quindi replicato le pr-curve per i quattro classificatori presentati nel paper, vale a dire il classificatore \textbf{ipr}, \textbf{coverage}, \textbf{knn} e \textbf{parzen}, per due differenti distribuzioni di dati, in particolare distribuzioni normali con media in \(0\) per i dati reali  e \(1/\sqrt{dim}\) e \(3/\sqrt{dim}\) per i dati generati (\(dim = 64\)).\ 
I classificatori hanno operato su un dataset di 20000 elementi, con 10000 elementi per ciascuna classe. Per gli esperimenti condotti i dati sono stati divisi in training e test set sia operando uno split del 50\% (ovvero 5000 punti effettivi per classe) sia senza split.\
Sono stati inoltre scelti due valori di \( k \) ovvero \(k = 4\) e \(k = \sqrt{n}\) (dove \(n\) è il numero di punti nel training set).\
Osservando i risultati del paper ci si aspetta che delle quattro pr-curve generate, la coverage-curve sia la più estrema, ovvero quella che produce un risultato migliore (più vicino al classificatore ottimale).\
Un'altra proprietà attesa è la simmetria delle curve rispetto alla diagonale, questo è dovuto al tipo di distribuzione dei dati utilizzata e al fatto che i training set fossero bilanciati.\
Dei test preliminari hanno poi mostrato fondamentale la scelta del range di valori e degli step per quanto riguarda la variabile \( \lambda \) (ovvero il parametro che regola la trade-off tra precision e recall). 
Come consigliato da \cite{unifying precision-recall}, il range di valori è stato generato dalla formula \( \tan(\pi/2 * i/(g+1)) \) con \( i \in [1, g] \) e \( g = 1001\) il numero di valori generati. Questa trasformazione consente di esplorare diverse scale di \(\lambda\) con una densità variabile: i valori crescono rapidamente da 0 a 1, variano lentamente vicino a \(\pi/2\)​, e infine aumentano rapidamente verso l'infinito. Questa caratteristica rende la funzione adatta per analizzare con precisione le transizioni critiche della PR-curve in regioni chiave, bilanciando una copertura fine e una rapida esplorazione delle estremità. 
In fase sperimentale sono state utilizzate altre funzioni per coprire il range di valori di \(\lambda\), ma la funzione sopra descritta è risultata la più adatta per l'analisi delle curve, e quella che ha prodotto i risultati più simili a quelli presenti in letteratura.

\section{Real World Dataset}

Come anticipato nell'introduzione di questo capitolo, oltre agli esperimenti condotti in ambienti controllati, 
regolati e basati su dati sintetici, è fondamentale analizzare il comportamento delle metriche in condizioni reali, 
ovvero su dataset rappresentativi di problemi pratici. Questa fase di sperimentazione consente di testare l'applicabilità 
delle metriche in contesti che vanno oltre l’ambito strettamente numerico e teorico, avvicinandosi alle condizioni operative 
in cui tali strumenti dovrebbero operare. In particolare, l’obiettivo finale delle metriche studiate è proprio quello 
di fornire un supporto concreto nell’analisi della qualità dei dati generati, facilitando l’integrazione delle reti generative in applicazioni pratiche.

Gli esperimenti sui dati reali sono stati condotti su due dataset distinti: un set di immagini raffiguranti farfalle 
e una collezione di partiture musicali di Alessandro Scarlatti, compositore rappresentativo della musica barocca. 
Questi dataset presentano specificità intrinseche che richiedono l’estrazione di caratteristiche rilevanti dal dominio dei dati (in particolare per le immagini utilizzare i \texttt{raw data} sarebbe improponibile data la loro dimensione). 
Per le immagini delle farfalle si è scelto di lavorare con feature basilari, come istogrammi di colore e saturazione, per valutare l’abilità delle metriche nel rilevare differenze qualitative senza fare ricorso a rappresentazioni complesse o specifiche del dominio.
Nel caso delle partiture musicali, invece, le caratteristiche estratte sono state più mirate e informate dal dominio della musica barocca seguendo quanto descritto nella letteratura \cite{music paper}. 
Sono state utilizzate, ad esempio, informazioni di carattere ritmico e tonali. Questo approccio permette di valutare le metriche su dati complessi con maggiore precisione.

Un ulteriore strumento di analisi utilizzato in questo contesto è la \textbf{Kernel Density Estimation} (\textbf{KDE}), che si è dimostrata particolarmente utile per ottenere una stima non parametrica della distribuzione dei dati. 
La \textbf{KDE} permette di visualizzare come i dati siano distribuiti nel loro spazio delle caratteristiche, fornendo così un quadro più completo delle relazioni tra i campioni reali e quelli generati. 
Questa informazione è cruciale per interpretare meglio il comportamento delle \textbf{metriche}, soprattutto quando si cerca di identificare regioni di alta o bassa densità che potrebbero indicare rispettivamente dati generati di alta qualità o outlier.

Infine, un obiettivo centrale di questa fase sperimentale è quello di verificare l'efficacia delle metriche nel discriminare dati generati di alta qualità da dati generati di bassa qualità, 
fungendo così da filtro ultimo per le reti generative. In questa ottica, le metriche potrebbero operare come strumento di selezione, scartando i dati che non soddisfano determinati standard di qualità e potenzialmente indicando i campioni da rigenerare.

\subsection{Butterflies}

Gli esperimenti condotti sul dataset di immagini di farfalle si sono basati su un'analisi semplice ma efficace delle caratteristiche visive, sfruttando estrattori di caratteristiche basati su istogrammi. In particolare, per ogni immagine sono stati calcolati sei tipi di istogrammi: 
\begin{itemize}
    \item \textbf{hue histogram}
    \item \textbf{saturation histogram}
    \item \textbf{value histogram}
    \item \textbf{grayscale histogram} (diverso dal value histogram, in questo caso abbiamo una combinazione lineare dei valori RGB)
    \item \textbf{rgb histogram}
    \item \textbf{hsv histogram}
\end{itemize}
Ogni istogramma è stato generato utilizzando 256 \textbf{bin} (per un totale di 256*3 bin per le rappresentazioni \textbf{RGB} e \textbf{HSV}), con l’obiettivo di catturare le distribuzioni dei valori cromatici e di intensità nelle immagini.

Per la rilevazione dei \textbf{falsi positivi}, è stato utilizzato un classificatore analogo a quello impiegato negli esperimenti sulle \textbf{IPR-curve}. In questo caso, le differenze tra istogrammi sono state misurate utilizzando sia la norma \(l_1\) che la norma \(l_2\)​ come funzioni di distanza. Il classificatore ha operato su un \textbf{k-nearest neighbors} (\textbf{k-NN}) con \(k=3\) e \(k=\sqrt{n}\), dove \(n\) è il numero di punti nel training set.
​Per valutare le prestazioni del classificatore, gli esperimenti sono stati condotti utilizzando diversi schemi di divisione dei dati: uno split 80-20 per il \textbf{training} e il \textbf{test}, e un approccio senza divisione, in cui tutti i dati venivano considerati come parte di un unico set per la classificazione.

\subsection{Scarlatti}




